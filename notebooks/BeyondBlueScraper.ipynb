{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-13T21:55:39.901338Z",
     "start_time": "2025-07-13T18:59:24.954843Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import calendar\n",
    "import json\n",
    "import emoji\n",
    "import emot\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "# Optional: load environment variables (not strictly needed here)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load kaomoji mapping if present\n",
    "KAOMOJI_FILE = \"kaomoji_to_text.json\"\n",
    "if os.path.exists(KAOMOJI_FILE):\n",
    "    with open(KAOMOJI_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        kaomoji_to_text = json.load(f)\n",
    "else:\n",
    "    kaomoji_to_text = {}\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUTPUT_DIR = \"data/beyondblue_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_post_date(raw_date: str) -> str:\n",
    "    \"\"\"Convert relative or weekday dates into YYYY-MM-DD.\"\"\"\n",
    "    today = datetime.now()\n",
    "    weekdays = [d.lower() for d in calendar.day_name]\n",
    "    s = raw_date.strip().lower()\n",
    "\n",
    "    if s in weekdays:\n",
    "        delta = (today.weekday() - weekdays.index(s)) % 7\n",
    "        return (today - timedelta(days=delta)).strftime(\"%Y-%m-%d\")\n",
    "    if \"week\" in s:\n",
    "        n = int(re.search(r\"(\\d+)\", s).group(1)) if re.search(r\"\\d+\", s) else 1\n",
    "        return (today - timedelta(weeks=n)).strftime(\"%Y-%m-%d\")\n",
    "    if \"month\" in s:\n",
    "        n = int(re.search(r\"(\\d+)\", s).group(1)) if re.search(r\"\\d+\", s) else 1\n",
    "        return (today - timedelta(days=30*n)).strftime(\"%Y-%m-%d\")\n",
    "    try:\n",
    "        return datetime.strptime(raw_date, \"%d-%m-%Y\").strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def convert_emojis_emoticons(text: str) -> str:\n",
    "    \"\"\"Replace kaomoji, emoticons, and emojis with text labels.\"\"\"\n",
    "    for k, v in kaomoji_to_text.items():\n",
    "        text = text.replace(k, f\" {v} \")\n",
    "    e = emot.core.emot()\n",
    "    emo = e.emoticons(text)\n",
    "    for orig, mean in zip(emo[\"value\"], emo[\"mean\"]):\n",
    "        text = text.replace(orig, f\" {mean} \")\n",
    "    text = emoji.demojize(text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "def comment_scraping(url: str, max_pages: int = 3, wait: int = 10) -> str:\n",
    "    \"\"\"Scrape up to `max_pages` of comments from a post page.\"\"\"\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.set_page_load_timeout(30)\n",
    "    comments = []\n",
    "    try:\n",
    "        page_url = url\n",
    "        for _ in range(max_pages):\n",
    "            try:\n",
    "                driver.get(page_url)\n",
    "                WebDriverWait(driver, wait).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"linear-message-list\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                break\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            section = soup.find(\"div\", class_=\"linear-message-list\")\n",
    "            if not section:\n",
    "                break\n",
    "            for msg in section.find_all(\"div\", recursive=False):\n",
    "                txt = msg.get_text(\" \", strip=True)\n",
    "                comments.append(convert_emojis_emoticons(txt))\n",
    "            nxt = soup.find(\"a\", rel=\"next\")\n",
    "            if not nxt or not nxt.get(\"href\"):\n",
    "                break\n",
    "            page_url = nxt[\"href\"]\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return \" ||| \".join(comments)\n",
    "\n",
    "\n",
    "def beyondblue_scraping(tag: str, start_url: str, pages: int = 20):\n",
    "    \"\"\"Scrape posts and comments for a given Beyond Blue forum tag.\"\"\"\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.set_page_load_timeout(30)\n",
    "    all_posts = []\n",
    "    url = start_url\n",
    "    try:\n",
    "        for p in tqdm(range(1, pages + 1), desc=f\"Scraping {tag}\"):\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"custom-message-list\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                continue\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            container = soup.find(\"div\", class_=\"custom-message-list all-discussions\")\n",
    "            if not container:\n",
    "                break\n",
    "            for art in container.find_all(\"article\"):\n",
    "                aside = art.find(\"aside\")\n",
    "                cat_div = aside.find(\"div\", class_=\"custom-tile-category-content\") if aside else None\n",
    "                post_cat = cat_div.find(\"a\").text.strip() if cat_div and cat_div.find(\"a\") else \"\"\n",
    "                raw_date = cat_div.find(\"time\").text.strip() if cat_div and cat_div.find(\"time\") else \"\"\n",
    "                date = parse_post_date(raw_date)\n",
    "                link_el = art.find(\"h3\").find_all(\"a\")[1]\n",
    "                post_link = link_el[\"href\"]\n",
    "                post_id = post_link.rstrip(\"/\").split(\"/\")[-1]\n",
    "                full_link = \"https://forums.beyondblue.org.au\" + post_link\n",
    "                title = convert_emojis_emoticons(link_el.text.strip())\n",
    "                body = art.find(\"p\", class_=\"body-text\")\n",
    "                content = convert_emojis_emoticons(body.text.strip()) if body else \"\"\n",
    "                auth_div = aside.find(\"div\", class_=\"custom-tile-author-info\") if aside else None\n",
    "                auth_a = auth_div.find(\"a\") if auth_div else None\n",
    "                author = auth_a.get_text(strip=True) if auth_a else \"\"\n",
    "                uid = auth_a[\"href\"].split(\"user-id/\")[-1] if auth_a and \"user-id/\" in auth_a[\"href\"] else \"\"\n",
    "                rep = art.find(\"li\", class_=\"custom-tile-replies\")\n",
    "                num_com = rep.find(\"b\").text.strip() if rep and rep.find(\"b\") else \"0\"\n",
    "                comms = comment_scraping(full_link, max_pages=3)\n",
    "                all_posts.append({\n",
    "                    \"Post ID\": post_id,\n",
    "                    \"Post Title\": title,\n",
    "                    \"Post Content\": content,\n",
    "                    \"Post Author\": author,\n",
    "                    \"User ID\": uid,\n",
    "                    \"Post Date\": date,\n",
    "                    \"Post Category\": post_cat,\n",
    "                    \"Number of Comments\": num_com,\n",
    "                    \"Comments\": comms\n",
    "                })\n",
    "            nxt_li = soup.find(\"li\", class_=\"lia-paging-page-next\")\n",
    "            if nxt_li and nxt_li.find(\"a\"):\n",
    "                url = nxt_li.find(\"a\")[\"href\"]\n",
    "            else:\n",
    "                break\n",
    "            if p % 10 == 0:\n",
    "                temp_df = pd.DataFrame(all_posts).drop_duplicates(subset=\"Post ID\")\n",
    "                temp_df.to_csv(f\"{OUTPUT_DIR}/{tag}_page{p}.csv\", index=False)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    df = pd.DataFrame(all_posts).drop_duplicates(subset=\"Post ID\")\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/{tag}_beyondblue_posts.csv\", index=False)\n",
    "    print(f\"Saved {len(df)} posts for {tag} to {OUTPUT_DIR}/{tag}_beyondblue_posts.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mental_health_urls = {\n",
    "        \"Sex_identity\": \"https://forums.beyondblue.org.au/t5/sexuality-and-gender-identity/bd-p/c1-sc4-b2?&sort=recent\",\n",
    "        \"Multiculture\":  \"https://forums.beyondblue.org.au/t5/multicultural-experiences/bd-p/c1-sc4-b3?&sort=recent\",\n",
    "        \"Grief_loss\":    \"https://forums.beyondblue.org.au/t5/grief-and-loss/bd-p/c1-sc4-b4?&sort=recent\"\n",
    "    }\n",
    "    for tag, addr in mental_health_urls.items():\n",
    "        try:\n",
    "            beyondblue_scraping(tag, addr, pages=50)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {tag}: {e}\")\n",
    "            continue\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sex_identity: 100%|██████████| 50/50 [1:32:16<00:00, 110.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 posts for Sex_identity to data/beyondblue_data/Sex_identity_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Multiculture:  50%|█████     | 25/50 [47:52<47:52, 114.90s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 256 posts for Multiculture to data/beyondblue_data/Multiculture_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Grief_loss:  38%|███▊      | 19/50 [35:28<57:52, 112.00s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Grief_loss: Message: unknown error: net::ERR_INTERNET_DISCONNECTED\n",
      "  (Session info: chrome=138.0.7204.101)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xb844a3+62419]\n",
      "\tGetHandleVerifier [0x0xb844e4+62484]\n",
      "\t(No symbol) [0x0x9c2133]\n",
      "\t(No symbol) [0x0x9bf860]\n",
      "\t(No symbol) [0x0x9b30e2]\n",
      "\t(No symbol) [0x0x9b4b18]\n",
      "\t(No symbol) [0x0x9b3378]\n",
      "\t(No symbol) [0x0x9b2eb3]\n",
      "\t(No symbol) [0x0x9b2bc1]\n",
      "\t(No symbol) [0x0x9b0b64]\n",
      "\t(No symbol) [0x0x9b150b]\n",
      "\t(No symbol) [0x0x9c5b5e]\n",
      "\t(No symbol) [0x0xa51447]\n",
      "\t(No symbol) [0x0xa2f46c]\n",
      "\t(No symbol) [0x0xa5087a]\n",
      "\t(No symbol) [0x0xa2f266]\n",
      "\t(No symbol) [0x0x9fe852]\n",
      "\t(No symbol) [0x0x9ff6f4]\n",
      "\tGetHandleVerifier [0x0xdf4793+2619075]\n",
      "\tGetHandleVerifier [0x0xdefbaa+2599642]\n",
      "\tGetHandleVerifier [0x0xbab04a+221050]\n",
      "\tGetHandleVerifier [0x0xb9b2c8+156152]\n",
      "\tGetHandleVerifier [0x0xba1c7d+183213]\n",
      "\tGetHandleVerifier [0x0xb8c388+94904]\n",
      "\tGetHandleVerifier [0x0xb8c512+95298]\n",
      "\tGetHandleVerifier [0x0xb7766a+9626]\n",
      "\tBaseThreadInitThunk [0x0x74fd5d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7702d1ab+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x7702d131+561]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T02:14:16.357906Z",
     "start_time": "2025-07-19T01:07:06.280260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import calendar\n",
    "import json\n",
    "import emoji\n",
    "import emot\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import unicodedata\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Kaomoji mapping (if present)\n",
    "KAOMOJI_FILE = \"kaomoji_to_text.json\"\n",
    "if os.path.exists(KAOMOJI_FILE):\n",
    "    with open(KAOMOJI_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        kaomoji_to_text = json.load(f)\n",
    "else:\n",
    "    kaomoji_to_text = {}\n",
    "\n",
    "OUTPUT_DIR = \"Data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\"Remove unicode control characters, normalize whitespace, and strip.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = ''.join(c for c in s if unicodedata.category(c)[0] != 'C' and c != '\\uFFFD')\n",
    "    return s.replace('\\xa0', ' ').replace('\\u200e', '').strip()\n",
    "\n",
    "def standardize_date(raw_date: str) -> str:\n",
    "    \"\"\"Attempt to parse and return date as 'YYYY-MM-DD HH:MM' or 'YYYY-MM-DD'.\"\"\"\n",
    "    s = clean_text(raw_date)\n",
    "    today = datetime.now()\n",
    "    weekdays = [d.lower() for d in calendar.day_name]\n",
    "    s_lower = s.lower()\n",
    "\n",
    "    # Handle relative dates\n",
    "    if s_lower in weekdays:\n",
    "        delta = (today.weekday() - weekdays.index(s_lower)) % 7\n",
    "        return (today - timedelta(days=delta)).strftime(\"%Y-%m-%d\")\n",
    "    if \"yesterday\" in s_lower:\n",
    "        return (today - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    if \"today\" in s_lower:\n",
    "        return today.strftime(\"%Y-%m-%d\")\n",
    "    if \"week\" in s_lower:\n",
    "        n = int(re.search(r\"(\\d+)\", s_lower).group(1)) if re.search(r\"\\d+\", s_lower) else 1\n",
    "        return (today - timedelta(weeks=n)).strftime(\"%Y-%m-%d\")\n",
    "    if \"month\" in s_lower:\n",
    "        n = int(re.search(r\"(\\d+)\", s_lower).group(1)) if re.search(r\"\\d+\", s_lower) else 1\n",
    "        return (today - timedelta(days=30*n)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Try various common formats\n",
    "    fmts = [\n",
    "        \"%d-%m-%Y %I:%M %p\", \"%d-%m-%Y %H:%M\", \"%d-%m-%Y\", \"%Y-%m-%d\",\n",
    "        \"%d/%m/%Y\", \"%d/%m/%Y %H:%M\", \"%Y-%m-%d %H:%M\"\n",
    "    ]\n",
    "    for fmt in fmts:\n",
    "        try:\n",
    "            dt = datetime.strptime(s, fmt)\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    # Try to extract date and time by regex\n",
    "    m = re.match(r\"(\\d{2})-(\\d{2})-(\\d{4})\\s*([0-9]{1,2}):([0-9]{2})\", s)\n",
    "    if m:\n",
    "        return f\"{m.group(3)}-{m.group(2)}-{m.group(1)} {m.group(4)}:{m.group(5)}\"\n",
    "    # Only date part\n",
    "    m = re.match(r\"(\\d{2})-(\\d{2})-(\\d{4})\", s)\n",
    "    if m:\n",
    "        return f\"{m.group(3)}-{m.group(2)}-{m.group(1)}\"\n",
    "    # fallback: return cleaned string\n",
    "    return s\n",
    "\n",
    "def convert_emojis_emoticons(text: str) -> str:\n",
    "    text = clean_text(text)\n",
    "    for k, v in kaomoji_to_text.items():\n",
    "        text = text.replace(k, f\" {v} \")\n",
    "    e = emot.core.emot()\n",
    "    emo = e.emoticons(text)\n",
    "    for orig, mean in zip(emo[\"value\"], emo[\"mean\"]):\n",
    "        text = text.replace(orig, f\" {mean} \")\n",
    "    text = emoji.demojize(text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def get_existing_ids(filepath, id_column):\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            return set(df[id_column].astype(str))\n",
    "        except Exception:\n",
    "            return set()\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "def make_comment_id(msg, post_id, comment_content):\n",
    "    \"\"\"Use HTML id if available; otherwise, use a hash of content for stability.\"\"\"\n",
    "    comment_id = (msg.get('data-message-id') or msg.get('id') or '').strip()\n",
    "    if comment_id and comment_id.lower() not in ['lineardisplaymessageviewwrapper', '']:\n",
    "        return comment_id\n",
    "    # Fallback: hash-based synthetic ID\n",
    "    hash_part = hashlib.sha256(comment_content.encode('utf-8')).hexdigest()[:10]\n",
    "    return f\"{post_id}_c{hash_part}\"\n",
    "\n",
    "def extract_comment_date(msg):\n",
    "    date_elem = msg.find(\"span\", class_=\"local-friendly-date\")\n",
    "    if date_elem:\n",
    "        if date_elem.has_attr('title') and date_elem['title'].strip():\n",
    "            return standardize_date(date_elem['title'])\n",
    "        elif date_elem.text.strip():\n",
    "            return standardize_date(date_elem.text)\n",
    "    datetime_elem = msg.find(\"span\", class_=\"DateTime\")\n",
    "    if datetime_elem and datetime_elem.text.strip():\n",
    "        return standardize_date(datetime_elem.text)\n",
    "    if msg.has_attr('data-message-timestamp'):\n",
    "        try:\n",
    "            ts = int(msg['data-message-timestamp'])\n",
    "            return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M')\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ''\n",
    "\n",
    "def comment_scraping(driver, post_url, post_id, category, max_comments=50, retry=3, polite_delay=1):\n",
    "    comments = []\n",
    "    comments_csv = os.path.join(OUTPUT_DIR, f\"comments_{category}.csv\")\n",
    "    existing_comment_ids = get_existing_ids(comments_csv, \"Comment ID\")\n",
    "\n",
    "    url = post_url\n",
    "    scraped = 0\n",
    "    total_comments = 0\n",
    "    for page in range(1, 100):  # Arbitrary large page count, will break on next not found\n",
    "        if scraped >= max_comments:\n",
    "            break\n",
    "        success = False\n",
    "        for attempt in range(retry):\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"linear-message-list\"))\n",
    "                )\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                success = True\n",
    "                break\n",
    "            except Exception:\n",
    "                time.sleep(2)\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        section = soup.find(\"div\", class_=\"linear-message-list\")\n",
    "        if not section:\n",
    "            break\n",
    "\n",
    "        msgs = section.find_all(\"div\", recursive=False)\n",
    "        if page == 1:\n",
    "            header = soup.find(\"h2\", class_=\"lia-message-thread-reply-count\")\n",
    "            if header:\n",
    "                try:\n",
    "                    total_comments = int(re.sub(r'[^\\d]', '', header.text))\n",
    "                except Exception:\n",
    "                    total_comments = 0\n",
    "            else:\n",
    "                total_comments = len(msgs)\n",
    "\n",
    "        for msg in msgs:\n",
    "            if scraped >= max_comments:\n",
    "                break\n",
    "            content_elem = msg.find(\"div\", class_=\"lia-message-body-content\")\n",
    "            comment_content = convert_emojis_emoticons(content_elem.get_text(\"\\n\", strip=True)) if content_elem else \"\"\n",
    "            comment_id = make_comment_id(msg, post_id, comment_content)\n",
    "            if comment_id in existing_comment_ids:\n",
    "                continue\n",
    "\n",
    "            author_elem = msg.find(\"a\", class_=\"lia-user-name-link\")\n",
    "            comment_author = clean_text(author_elem.get_text(strip=True)) if author_elem else \"\"\n",
    "            comment_date = extract_comment_date(msg)\n",
    "            support_elem = msg.find(\"span\", class_=\"lia-component-kudos-widget-message-kudos-count\") \\\n",
    "                or msg.find(\"span\", class_=\"kudos-count-link\")\n",
    "            comment_support = support_elem.text.strip() if support_elem else \"0\"\n",
    "            comment_row = {\n",
    "                \"Comment ID\": comment_id,\n",
    "                \"Post ID\": post_id,\n",
    "                \"Category\": category,\n",
    "                \"Comment Author\": comment_author,\n",
    "                \"Comment Date\": comment_date,\n",
    "                \"Comment Content\": comment_content,\n",
    "                \"Comment Support\": comment_support,\n",
    "                \"Post URL\": post_url\n",
    "            }\n",
    "            comments.append(comment_row)\n",
    "            existing_comment_ids.add(comment_id)  # Deduplicate within run\n",
    "            scraped += 1\n",
    "        # Pagination: find next page for comments\n",
    "        nxt = soup.find(\"a\", rel=\"next\")\n",
    "        if not nxt or not nxt.get(\"href\"):\n",
    "            break\n",
    "        url = \"https://forums.beyondblue.org.au\" + nxt[\"href\"] if nxt[\"href\"].startswith(\"/\") else nxt[\"href\"]\n",
    "        time.sleep(polite_delay)\n",
    "    # Sort comments by date before returning (if possible)\n",
    "    comments = sorted(comments, key=lambda c: c[\"Comment Date\"])\n",
    "    return comments, total_comments\n",
    "\n",
    "def beyondblue_scraping(tag: str, start_url: str, pages: int = 20, polite_delay=2):\n",
    "    posts_csv = os.path.join(OUTPUT_DIR, f\"posts_{tag}.csv\")\n",
    "    comments_csv = os.path.join(OUTPUT_DIR, f\"comments_{tag}.csv\")\n",
    "    existing_post_ids = get_existing_ids(posts_csv, \"Post ID\")\n",
    "    existing_comment_ids = get_existing_ids(comments_csv, \"Comment ID\")\n",
    "    all_posts = []\n",
    "    all_comments = []\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.set_page_load_timeout(30)\n",
    "    url = start_url\n",
    "    try:\n",
    "        for p in tqdm(range(1, pages + 1), desc=f\"Scraping {tag}\"):\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"custom-message-list\"))\n",
    "                    )\n",
    "                    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                    break\n",
    "                except Exception:\n",
    "                    if attempt == 2:\n",
    "                        print(f\"Failed to load page {url}\")\n",
    "                        return\n",
    "                    time.sleep(2)\n",
    "            container = soup.find(\"div\", class_=\"custom-message-list all-discussions\")\n",
    "            if not container:\n",
    "                break\n",
    "            for art in container.find_all(\"article\"):\n",
    "                aside = art.find(\"aside\")\n",
    "                cat_div = aside.find(\"div\", class_=\"custom-tile-category-content\") if aside else None\n",
    "                post_cat = clean_text(cat_div.find(\"a\").text.strip()) if cat_div and cat_div.find(\"a\") else tag\n",
    "                raw_date = cat_div.find(\"time\").text.strip() if cat_div and cat_div.find(\"time\") else \"\"\n",
    "                date = standardize_date(raw_date)\n",
    "                h3 = art.find(\"h3\")\n",
    "                link_els = h3.find_all(\"a\") if h3 else []\n",
    "                link_el = link_els[1] if len(link_els) > 1 else (link_els[0] if link_els else None)\n",
    "                post_link = link_el[\"href\"] if link_el and link_el.has_attr(\"href\") else \"\"\n",
    "                post_id = post_link.rstrip(\"/\").split(\"/\")[-1] if post_link else \"\"\n",
    "                if not post_id or post_id in existing_post_ids:\n",
    "                    continue\n",
    "                full_link = \"https://forums.beyondblue.org.au\" + post_link if post_link.startswith(\"/\") else post_link\n",
    "                title = convert_emojis_emoticons(link_el.text.strip()) if link_el else \"\"\n",
    "                body = art.find(\"p\", class_=\"body-text\")\n",
    "                content = convert_emojis_emoticons(body.text.strip()) if body else \"\"\n",
    "                auth_div = aside.find(\"div\", class_=\"custom-tile-author-info\") if aside else None\n",
    "                auth_a = auth_div.find(\"a\") if auth_div else None\n",
    "                author = clean_text(auth_a.get_text(strip=True)) if auth_a else \"\"\n",
    "                rep = art.find(\"li\", class_=\"custom-tile-replies\")\n",
    "                num_com = rep.find(\"b\").text.strip() if rep and rep.find(\"b\") else \"0\"\n",
    "                post_support = \"0\"\n",
    "\n",
    "                post_content = \"\"\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        driver.execute_script(\"window.open('');\")\n",
    "                        driver.switch_to.window(driver.window_handles[1])\n",
    "                        driver.get(full_link)\n",
    "                        WebDriverWait(driver, 10).until(\n",
    "                            EC.presence_of_element_located((By.CLASS_NAME, \"lia-message-body-content\"))\n",
    "                        )\n",
    "                        post_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                        content_elems = post_soup.find_all(\"div\", class_=\"lia-message-body-content\")\n",
    "                        post_content = \"\\n\".join([convert_emojis_emoticons(elem.get_text(\"\\n\", strip=True)) for elem in content_elems if elem.get_text(strip=True)])\n",
    "                        # Extract support count from the full post page\n",
    "                        support_span = post_soup.find(\"span\", class_=\"lia-component-kudos-widget-message-kudos-count\") \\\n",
    "                            or post_soup.find(\"span\", class_=\"kudos-count-link\")\n",
    "                        if support_span:\n",
    "                            post_support = support_span.text.strip()\n",
    "                        else:\n",
    "                            post_support = \"0\"\n",
    "                        comments, total_comment_count = comment_scraping(driver, full_link, post_id, tag, max_comments=50)\n",
    "                        # Deduplicate comments (skip already scraped)\n",
    "                        new_comments = []\n",
    "                        for c in comments:\n",
    "                            if c[\"Comment ID\"] not in existing_comment_ids:\n",
    "                                new_comments.append(c)\n",
    "                                existing_comment_ids.add(c[\"Comment ID\"])\n",
    "                        all_comments.extend(new_comments)\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(driver.window_handles[0])\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt == 2:\n",
    "                            print(f\"Failed to extract post page {full_link}: {e}\")\n",
    "                            try:\n",
    "                                driver.close()\n",
    "                                driver.switch_to.window(driver.window_handles[0])\n",
    "                            except: pass\n",
    "                            post_content = content\n",
    "                            total_comment_count = num_com\n",
    "                            post_support = \"0\"\n",
    "                        else:\n",
    "                            time.sleep(2)\n",
    "                post_data = {\n",
    "                    \"Post ID\": post_id,\n",
    "                    \"Category\": post_cat,\n",
    "                    \"Post Title\": title,\n",
    "                    \"Post Author\": author,\n",
    "                    \"Post Date\": date,\n",
    "                    \"Post Content\": post_content,\n",
    "                    \"Support Count\": post_support,\n",
    "                    \"Total Number of Comments\": total_comment_count,\n",
    "                    \"Post URL\": full_link\n",
    "                }\n",
    "                all_posts.append(post_data)\n",
    "                existing_post_ids.add(post_id)\n",
    "                time.sleep(polite_delay)\n",
    "            nxt_li = soup.find(\"li\", class_=\"lia-paging-page-next\")\n",
    "            if nxt_li and nxt_li.find(\"a\"):\n",
    "                next_href = nxt_li.find(\"a\")[\"href\"]\n",
    "                url = \"https://forums.beyondblue.org.au\" + next_href if next_href.startswith(\"/\") else next_href\n",
    "            else:\n",
    "                break\n",
    "            if p % 5 == 0:\n",
    "                dfp = pd.DataFrame(all_posts)\n",
    "                dfp.sort_values(by=\"Post Date\", inplace=True)\n",
    "                dfp.to_csv(posts_csv, index=False)\n",
    "                dfc = pd.DataFrame(all_comments)\n",
    "                dfc.sort_values(by=\"Comment Date\", inplace=True)\n",
    "                dfc.to_csv(comments_csv, index=False)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    # Final save, sorted by date\n",
    "    if all_posts:\n",
    "        dfp = pd.DataFrame(all_posts)\n",
    "        if os.path.exists(posts_csv):\n",
    "            dfp_existing = pd.read_csv(posts_csv)\n",
    "            dfp = pd.concat([dfp_existing, dfp], ignore_index=True)\n",
    "            dfp.drop_duplicates(subset=[\"Post ID\"], inplace=True)\n",
    "        dfp.sort_values(by=\"Post Date\", inplace=True)\n",
    "        dfp.to_csv(posts_csv, index=False)\n",
    "        print(f\"Saved {len(dfp)} posts to {posts_csv}\")\n",
    "    if all_comments:\n",
    "        dfc = pd.DataFrame(all_comments)\n",
    "        if os.path.exists(comments_csv):\n",
    "            dfc_existing = pd.read_csv(comments_csv)\n",
    "            dfc = pd.concat([dfc_existing, dfc], ignore_index=True)\n",
    "            dfc.drop_duplicates(subset=[\"Comment ID\"], inplace=True)\n",
    "        dfc.sort_values(by=\"Comment Date\", inplace=True)\n",
    "        dfc.to_csv(comments_csv, index=False)\n",
    "        print(f\"Saved {len(dfc)} comments to {comments_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mental_health_urls = {\n",
    "        \"anxiety\":      \"https://forums.beyondblue.org.au/t5/anxiety/bd-p/c1-sc2-b1?&sort=recent\",\n",
    "        \"depression\":   \"https://forums.beyondblue.org.au/t5/depression/bd-p/c1-sc2-b2?&sort=recent\",\n",
    "        \"ptsd_trauma\":  \"https://forums.beyondblue.org.au/t5/ptsd-and-trauma/bd-p/c1-sc2-b3?&sort=recent\",\n",
    "        \"suicidal_selfharm\": \"https://forums.beyondblue.org.au/t5/suicidal-thoughts-and-self-harm/bd-p/c1-sc2-b4?&sort=recent\",\n",
    "        \"staying_well\": \"https://forums.beyondblue.org.au/t5/staying-well/bd-p/c1-sc3-b1?&sort=recent\",\n",
    "        \"treatments\":   \"https://forums.beyondblue.org.au/t5/treatments-health-professionals/bd-p/c1-sc3-b2?&sort=recent\",\n",
    "        \"relationships\":\"https://forums.beyondblue.org.au/t5/relationship-and-family-issues/bd-p/c1-sc3-b3?&sort=recent\",\n",
    "        \"supporting_friends\": \"https://forums.beyondblue.org.au/t5/supporting-family-and-friends/bd-p/c1-sc3-b4?&sort=recent\",\n",
    "        \"long_term_support\":  \"https://forums.beyondblue.org.au/t5/long-term-support-over-the/bd-p/c1-sc3-b5?&sort=recent\",\n",
    "        \"young_people\": \"https://forums.beyondblue.org.au/t5/young-people/bd-p/c1-sc4-b1?&sort=recent\",\n",
    "        \"Sex_identity\": \"https://forums.beyondblue.org.au/t5/sexuality-and-gender-identity/bd-p/c1-sc4-b2?&sort=recent\",\n",
    "        \"Multiculture\":  \"https://forums.beyondblue.org.au/t5/multicultural-experiences/bd-p/c1-sc4-b3?&sort=recent\",\n",
    "        \"Grief_loss\":    \"https://forums.beyondblue.org.au/t5/grief-and-loss/bd-p/c1-sc4-b4?&sort=recent\"\n",
    "    }\n",
    "    for tag, addr in mental_health_urls.items():\n",
    "        try:\n",
    "            beyondblue_scraping(tag, addr, pages=100)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {tag}: {e}\")\n",
    "            continue"
   ],
   "id": "5caae4818e77e09e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping anxiety: 100%|██████████| 100/100 [44:24<00:00, 26.64s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 904 posts to Data\\posts_anxiety.csv\n",
      "Saved 452 comments to Data\\comments_anxiety.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping depression:  23%|██▎       | 23/100 [22:32<1:15:26, 58.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 374\u001B[39m\n\u001B[32m    372\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m tag, addr \u001B[38;5;129;01min\u001B[39;00m mental_health_urls.items():\n\u001B[32m    373\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m374\u001B[39m         \u001B[43mbeyondblue_scraping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtag\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maddr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpages\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    375\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    376\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError scraping \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtag\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 227\u001B[39m, in \u001B[36mbeyondblue_scraping\u001B[39m\u001B[34m(tag, start_url, pages, polite_delay)\u001B[39m\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m3\u001B[39m):\n\u001B[32m    226\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m227\u001B[39m         \u001B[43mdriver\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    228\u001B[39m         WebDriverWait(driver, \u001B[32m10\u001B[39m).until(\n\u001B[32m    229\u001B[39m             EC.presence_of_element_located((By.CLASS_NAME, \u001B[33m\"\u001B[39m\u001B[33mcustom-message-list\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m    230\u001B[39m         )\n\u001B[32m    231\u001B[39m         soup = BeautifulSoup(driver.page_source, \u001B[33m\"\u001B[39m\u001B[33mhtml.parser\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:479\u001B[39m, in \u001B[36mWebDriver.get\u001B[39m\u001B[34m(self, url)\u001B[39m\n\u001B[32m    461\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    462\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001B[39;00m\n\u001B[32m    463\u001B[39m \u001B[33;03m    tab.\u001B[39;00m\n\u001B[32m    464\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    477\u001B[39m \u001B[33;03m    >>> driver.get(\"https://example.com\")\u001B[39;00m\n\u001B[32m    478\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m479\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCommand\u001B[49m\u001B[43m.\u001B[49m\u001B[43mGET\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43murl\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:451\u001B[39m, in \u001B[36mWebDriver.execute\u001B[39m\u001B[34m(self, driver_command, params)\u001B[39m\n\u001B[32m    448\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33msessionId\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m params:\n\u001B[32m    449\u001B[39m         params[\u001B[33m\"\u001B[39m\u001B[33msessionId\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.session_id\n\u001B[32m--> \u001B[39m\u001B[32m451\u001B[39m response = \u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRemoteConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcommand_executor\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdriver_command\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m response:\n\u001B[32m    454\u001B[39m     \u001B[38;5;28mself\u001B[39m.error_handler.check_response(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001B[39m, in \u001B[36mRemoteConnection.execute\u001B[39m\u001B[34m(self, command, params)\u001B[39m\n\u001B[32m    402\u001B[39m trimmed = \u001B[38;5;28mself\u001B[39m._trim_large_entries(params)\n\u001B[32m    403\u001B[39m LOGGER.debug(\u001B[33m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, command_info[\u001B[32m0\u001B[39m], url, \u001B[38;5;28mstr\u001B[39m(trimmed))\n\u001B[32m--> \u001B[39m\u001B[32m404\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand_info\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001B[39m, in \u001B[36mRemoteConnection._request\u001B[39m\u001B[34m(self, method, url, body)\u001B[39m\n\u001B[32m    425\u001B[39m     body = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    427\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client_config.keep_alive:\n\u001B[32m--> \u001B[39m\u001B[32m428\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_client_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    429\u001B[39m     statuscode = response.status\n\u001B[32m    430\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001B[39m, in \u001B[36mRequestMethods.request\u001B[39m\u001B[34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001B[39m\n\u001B[32m    135\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request_encode_url(\n\u001B[32m    136\u001B[39m         method,\n\u001B[32m    137\u001B[39m         url,\n\u001B[32m   (...)\u001B[39m\u001B[32m    140\u001B[39m         **urlopen_kw,\n\u001B[32m    141\u001B[39m     )\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest_encode_body\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    144\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfields\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43murlopen_kw\u001B[49m\n\u001B[32m    145\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001B[39m, in \u001B[36mRequestMethods.request_encode_body\u001B[39m\u001B[34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001B[39m\n\u001B[32m    274\u001B[39m     extra_kw[\u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m].setdefault(\u001B[33m\"\u001B[39m\u001B[33mContent-Type\u001B[39m\u001B[33m\"\u001B[39m, content_type)\n\u001B[32m    276\u001B[39m extra_kw.update(urlopen_kw)\n\u001B[32m--> \u001B[39m\u001B[32m278\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mextra_kw\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\urllib3\\poolmanager.py:459\u001B[39m, in \u001B[36mPoolManager.urlopen\u001B[39m\u001B[34m(self, method, url, redirect, **kw)\u001B[39m\n\u001B[32m    457\u001B[39m     response = conn.urlopen(method, url, **kw)\n\u001B[32m    458\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m459\u001B[39m     response = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mu\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest_uri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    461\u001B[39m redirect_location = redirect \u001B[38;5;129;01mand\u001B[39;00m response.get_redirect_location()\n\u001B[32m    462\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m redirect_location:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    784\u001B[39m response_conn = conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    786\u001B[39m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m787\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    788\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    789\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    790\u001B[39m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    791\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    792\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    793\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    794\u001B[39m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    795\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    796\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    797\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    798\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    799\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    800\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    802\u001B[39m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[32m    803\u001B[39m clean_exit = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001B[39m, in \u001B[36mHTTPConnectionPool._make_request\u001B[39m\u001B[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[39m\n\u001B[32m    532\u001B[39m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[32m    533\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m534\u001B[39m     response = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    535\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    536\u001B[39m     \u001B[38;5;28mself\u001B[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\urllib3\\connection.py:565\u001B[39m, in \u001B[36mHTTPConnection.getresponse\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    562\u001B[39m _shutdown = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.sock, \u001B[33m\"\u001B[39m\u001B[33mshutdown\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    564\u001B[39m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m565\u001B[39m httplib_response = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    567\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    568\u001B[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Program Files\\Python312\\Lib\\http\\client.py:1423\u001B[39m, in \u001B[36mHTTPConnection.getresponse\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1421\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1422\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1423\u001B[39m         \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1424\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[32m   1425\u001B[39m         \u001B[38;5;28mself\u001B[39m.close()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Program Files\\Python312\\Lib\\http\\client.py:331\u001B[39m, in \u001B[36mHTTPResponse.begin\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    329\u001B[39m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m331\u001B[39m     version, status, reason = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    332\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m status != CONTINUE:\n\u001B[32m    333\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Program Files\\Python312\\Lib\\http\\client.py:292\u001B[39m, in \u001B[36mHTTPResponse._read_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    291\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m292\u001B[39m     line = \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[33m\"\u001B[39m\u001B[33miso-8859-1\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    293\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) > _MAXLINE:\n\u001B[32m    294\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[33m\"\u001B[39m\u001B[33mstatus line\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Program Files\\Python312\\Lib\\socket.py:707\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    706\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m707\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    708\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    709\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T18:31:24.961245Z",
     "start_time": "2025-07-19T03:02:22.821485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import calendar\n",
    "import json\n",
    "import emoji\n",
    "import emot\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import unicodedata\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "KAOMOJI_FILE = \"kaomoji_to_text.json\"\n",
    "if os.path.exists(KAOMOJI_FILE):\n",
    "    with open(KAOMOJI_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        kaomoji_to_text = json.load(f)\n",
    "else:\n",
    "    kaomoji_to_text = {}\n",
    "\n",
    "OUTPUT_DIR = \"Data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = ''.join(c for c in s if unicodedata.category(c)[0] != 'C' and c != '\\uFFFD')\n",
    "    return s.replace('\\xa0', ' ').replace('\\u200e', '').strip()\n",
    "\n",
    "def standardize_date(raw_date: str) -> str:\n",
    "    s = clean_text(raw_date)\n",
    "    today = datetime.now()\n",
    "    weekdays = [d.lower() for d in calendar.day_name]\n",
    "    s_lower = s.lower()\n",
    "\n",
    "    # Handle relative dates\n",
    "    if s_lower in weekdays:\n",
    "        delta = (today.weekday() - weekdays.index(s_lower)) % 7\n",
    "        return (today - timedelta(days=delta)).strftime(\"%d-%m-%Y\")\n",
    "    if \"yesterday\" in s_lower:\n",
    "        return (today - timedelta(days=1)).strftime(\"%d-%m-%Y\")\n",
    "    if \"today\" in s_lower:\n",
    "        return today.strftime(\"%d-%m-%Y\")\n",
    "    if \"week\" in s_lower:\n",
    "        n = int(re.search(r\"(\\d+)\", s_lower).group(1)) if re.search(r\"\\d+\", s_lower) else 1\n",
    "        return (today - timedelta(weeks=n)).strftime(\"%d-%m-%Y\")\n",
    "    if \"month\" in s_lower:\n",
    "        n = int(re.search(r\"(\\d+)\", s_lower).group(1)) if re.search(r\"\\d+\", s_lower) else 1\n",
    "        return (today - timedelta(days=30*n)).strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    # Try most relevant date formats\n",
    "    fmts = [\n",
    "        \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "        \"%Y-%m-%d %H:%M\",\n",
    "        \"%d-%m-%Y %H:%M\",\n",
    "        \"%d-%m-%Y %I:%M %p\",\n",
    "        \"%d/%m/%Y %H:%M\",\n",
    "        \"%d/%m/%Y %I:%M %p\",\n",
    "        \"%d-%m-%Y%H:%M\",\n",
    "        \"%Y-%m-%d%H:%M\",\n",
    "        \"%d/%m/%Y%H:%M\",\n",
    "    ]\n",
    "    for fmt in fmts:\n",
    "        try:\n",
    "            dt = datetime.strptime(s, fmt)\n",
    "            return dt.strftime(\"%d-%m-%Y\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    # Date only formats (no time info)\n",
    "    date_fmts = [\n",
    "        \"%d-%m-%Y\", \"%Y-%m-%d\", \"%d/%m/%Y\"\n",
    "    ]\n",
    "    for fmt in date_fmts:\n",
    "        try:\n",
    "            dt = datetime.strptime(s, fmt)\n",
    "            return dt.strftime(\"%d-%m-%Y\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    # Fallback: regex for non-standard\n",
    "    m = re.match(r\"(\\d{2})-(\\d{2})-(\\d{4})\", s)\n",
    "    if m:\n",
    "        return f\"{m.group(1)}-{m.group(2)}-{m.group(3)}\"\n",
    "    return s\n",
    "\n",
    "def convert_emojis_emoticons(text: str) -> str:\n",
    "    text = clean_text(text)\n",
    "    for k, v in kaomoji_to_text.items():\n",
    "        text = text.replace(k, f\" {v} \")\n",
    "    e = emot.core.emot()\n",
    "    emo = e.emoticons(text)\n",
    "    for orig, mean in zip(emo[\"value\"], emo[\"mean\"]):\n",
    "        text = text.replace(orig, f\" {mean} \")\n",
    "    text = emoji.demojize(text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def get_existing_ids(filepath, id_column):\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            return set(df[id_column].astype(str))\n",
    "        except Exception:\n",
    "            return set()\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "def make_comment_id(msg, post_id, comment_content):\n",
    "    comment_id = (msg.get('data-message-id') or msg.get('id') or '').strip()\n",
    "    if comment_id and comment_id.lower() not in ['lineardisplaymessageviewwrapper', '']:\n",
    "        return comment_id\n",
    "    hash_part = hashlib.sha256(comment_content.encode('utf-8')).hexdigest()[:10]\n",
    "    return f\"{post_id}_c{hash_part}\"\n",
    "\n",
    "def extract_comment_date(msg):\n",
    "    time_elem = msg.find(\"time\")\n",
    "    if time_elem:\n",
    "        if time_elem.has_attr('datetime'):\n",
    "            return standardize_date(time_elem['datetime'])\n",
    "        elif time_elem.has_attr('title'):\n",
    "            return standardize_date(time_elem['title'])\n",
    "        elif time_elem.text.strip():\n",
    "            return standardize_date(time_elem.text)\n",
    "    date_elem = msg.find(\"span\", class_=\"local-friendly-date\")\n",
    "    if date_elem:\n",
    "        if date_elem.has_attr('title') and date_elem['title'].strip():\n",
    "            return standardize_date(date_elem['title'])\n",
    "        elif date_elem.text.strip():\n",
    "            return standardize_date(date_elem.text)\n",
    "    datetime_elem = msg.find(\"span\", class_=\"DateTime\")\n",
    "    if datetime_elem and datetime_elem.text.strip():\n",
    "        return standardize_date(datetime_elem.text)\n",
    "    if msg.has_attr('data-message-timestamp'):\n",
    "        try:\n",
    "            ts = int(msg['data-message-timestamp'])\n",
    "            return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M')\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ''\n",
    "\n",
    "def comment_scraping(driver, post_url, post_id, category, max_comments=200, retry=3, polite_delay=1):\n",
    "    comments = []\n",
    "    comments_csv = os.path.join(OUTPUT_DIR, f\"comments_{category}.csv\")\n",
    "    existing_comment_ids = get_existing_ids(comments_csv, \"Comment ID\")\n",
    "    url = post_url\n",
    "    scraped = 0\n",
    "    for page in range(1, 100):  # will break on next not found\n",
    "        if scraped >= max_comments:\n",
    "            break\n",
    "        success = False\n",
    "        for attempt in range(retry):\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"linear-message-list\"))\n",
    "                )\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                success = True\n",
    "                break\n",
    "            except Exception:\n",
    "                time.sleep(2)\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        section = soup.find(\"div\", class_=\"linear-message-list\")\n",
    "        if not section:\n",
    "            break\n",
    "\n",
    "        msgs = section.find_all(\"div\", class_=\"lia-message-view-display\")\n",
    "        for msg in msgs:\n",
    "            if scraped >= max_comments:\n",
    "                break\n",
    "            content_elem = msg.find(\"div\", class_=\"lia-message-body-content\")\n",
    "            comment_content = convert_emojis_emoticons(content_elem.get_text(\"\\n\", strip=True)) if content_elem else \"\"\n",
    "            comment_id = make_comment_id(msg, post_id, comment_content)\n",
    "            if comment_id in existing_comment_ids:\n",
    "                continue\n",
    "            author_elem = msg.find(\"a\", class_=\"lia-user-name-link\")\n",
    "            comment_author = clean_text(author_elem.get_text(strip=True)) if author_elem else \"\"\n",
    "            comment_date = extract_comment_date(msg)\n",
    "            support_elem = msg.find(\"span\", {\"id\": re.compile(r\"^kudos-count-\")})\n",
    "            if not support_elem:\n",
    "                support_elem = msg.find(\"span\", class_=\"lia-component-kudos-widget-message-kudos-count\")\n",
    "            comment_support = support_elem.text.strip() if support_elem else \"0\"\n",
    "            comment_row = {\n",
    "                \"Comment ID\": comment_id,\n",
    "                \"Post ID\": post_id,\n",
    "                \"Category\": category,\n",
    "                \"Comment Author\": comment_author,\n",
    "                \"Comment Date\": comment_date,\n",
    "                \"Comment Content\": comment_content,\n",
    "                \"Comment Support\": comment_support,\n",
    "                \"Post URL\": post_url\n",
    "            }\n",
    "            comments.append(comment_row)\n",
    "            existing_comment_ids.add(comment_id)\n",
    "            scraped += 1\n",
    "        nxt = soup.find(\"a\", rel=\"next\")\n",
    "        if not nxt or not nxt.get(\"href\"):\n",
    "            break\n",
    "        url = \"https://forums.beyondblue.org.au\" + nxt[\"href\"] if nxt[\"href\"].startswith(\"/\") else nxt[\"href\"]\n",
    "        time.sleep(polite_delay)\n",
    "    comments = sorted(comments, key=lambda c: c[\"Comment Date\"])\n",
    "    return comments\n",
    "\n",
    "def beyondblue_scraping(tag: str, start_url: str, pages: int = 20, polite_delay=2):\n",
    "    posts_csv = os.path.join(OUTPUT_DIR, f\"posts_{tag}.csv\")\n",
    "    comments_csv = os.path.join(OUTPUT_DIR, f\"comments_{tag}.csv\")\n",
    "    existing_post_ids = get_existing_ids(posts_csv, \"Post ID\")\n",
    "    existing_comment_ids = get_existing_ids(comments_csv, \"Comment ID\")\n",
    "    all_posts = []\n",
    "    all_comments = []\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.set_page_load_timeout(30)\n",
    "    url = start_url\n",
    "    try:\n",
    "        for p in tqdm(range(1, pages + 1), desc=f\"Scraping {tag}\"):\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"custom-message-list\"))\n",
    "                    )\n",
    "                    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                    break\n",
    "                except Exception:\n",
    "                    if attempt == 2:\n",
    "                        print(f\"Failed to load page {url}\")\n",
    "                        return\n",
    "                    time.sleep(2)\n",
    "            container = soup.find(\"div\", class_=\"custom-message-list all-discussions\")\n",
    "            if not container:\n",
    "                break\n",
    "            for art in container.find_all(\"article\"):\n",
    "                aside = art.find(\"aside\")\n",
    "                cat_div = aside.find(\"div\", class_=\"custom-tile-category-content\") if aside else None\n",
    "                post_cat = clean_text(cat_div.find(\"a\").text.strip()) if cat_div and cat_div.find(\"a\") else tag\n",
    "                # --- DATE ---\n",
    "                time_el = cat_div.find(\"time\") if cat_div else None\n",
    "                if time_el and time_el.has_attr('datetime'):\n",
    "                    raw_date = time_el['datetime']\n",
    "                elif time_el and time_el.has_attr('title'):\n",
    "                    raw_date = time_el['title']\n",
    "                elif time_el:\n",
    "                    raw_date = time_el.text.strip()\n",
    "                else:\n",
    "                    raw_date = \"\"\n",
    "                date = standardize_date(raw_date)\n",
    "                # --- SUPPORT AND REPLIES ---\n",
    "                support_li = art.find(\"li\", class_=\"custom-tile-kudos\")\n",
    "                support_span = support_li.find(\"span\") if support_li else None\n",
    "                post_support = support_span.text.strip() if support_span else \"0\"\n",
    "\n",
    "                replies_li = art.find(\"li\", class_=\"custom-tile-replies\")\n",
    "                replies_b = replies_li.find(\"b\") if replies_li else None\n",
    "                total_comment_count = replies_b.text.strip() if replies_b else \"0\"\n",
    "                # --- POST TITLE/URL/CONTENT ---\n",
    "                h3 = art.find(\"h3\")\n",
    "                link_els = h3.find_all(\"a\") if h3 else []\n",
    "                link_el = link_els[1] if len(link_els) > 1 else (link_els[0] if link_els else None)\n",
    "                post_link = link_el[\"href\"] if link_el and link_el.has_attr(\"href\") else \"\"\n",
    "                post_id = post_link.rstrip(\"/\").split(\"/\")[-1] if post_link else \"\"\n",
    "                if not post_id or post_id in existing_post_ids:\n",
    "                    continue\n",
    "                full_link = \"https://forums.beyondblue.org.au\" + post_link if post_link.startswith(\"/\") else post_link\n",
    "                title = convert_emojis_emoticons(link_el.text.strip()) if link_el else \"\"\n",
    "                body = art.find(\"p\", class_=\"body-text\")\n",
    "                content = convert_emojis_emoticons(body.text.strip()) if body else \"\"\n",
    "                auth_div = aside.find(\"div\", class_=\"custom-tile-author-info\") if aside else None\n",
    "                auth_a = auth_div.find(\"a\") if auth_div else None\n",
    "                author = clean_text(auth_a.get_text(strip=True)) if auth_a else \"\"\n",
    "                post_content = content\n",
    "                # --- Get real post support from detail page ---\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        driver.execute_script(\"window.open('');\")\n",
    "                        driver.switch_to.window(driver.window_handles[1])\n",
    "                        driver.get(full_link)\n",
    "                        WebDriverWait(driver, 10).until(\n",
    "                            EC.presence_of_element_located((By.CLASS_NAME, \"lia-message-body-content\"))\n",
    "                        )\n",
    "                        post_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                        content_elems = post_soup.find_all(\"div\", class_=\"lia-message-body-content\")\n",
    "                        if content_elems:\n",
    "                            post_content = \"\\n\".join([convert_emojis_emoticons(elem.get_text(\"\\n\", strip=True)) for elem in content_elems if elem.get_text(strip=True)])\n",
    "                        # Get support count from first post in thread\n",
    "                        support_span_detail = post_soup.find(\"span\", {\"id\": re.compile(r\"^kudos-count-\")})\n",
    "                        if not support_span_detail:\n",
    "                            support_span_detail = post_soup.find(\"span\", class_=\"lia-component-kudos-widget-message-kudos-count\")\n",
    "                        if support_span_detail:\n",
    "                            post_support = support_span_detail.text.strip()\n",
    "                        # Get post date from detail if missing\n",
    "                        time_elem = post_soup.find(\"time\")\n",
    "                        if not date and time_elem:\n",
    "                            if time_elem.has_attr(\"datetime\"):\n",
    "                                date = standardize_date(time_elem[\"datetime\"])\n",
    "                            elif time_elem.has_attr(\"title\"):\n",
    "                                date = standardize_date(time_elem[\"title\"])\n",
    "                            else:\n",
    "                                date = standardize_date(time_elem.text.strip())\n",
    "                        # Scrape all comments (not for count, but real data)\n",
    "                        comments = comment_scraping(driver, full_link, post_id, tag, max_comments=200)\n",
    "                        new_comments = []\n",
    "                        for c in comments:\n",
    "                            if c[\"Comment ID\"] not in existing_comment_ids:\n",
    "                                new_comments.append(c)\n",
    "                                existing_comment_ids.add(c[\"Comment ID\"])\n",
    "                        all_comments.extend(new_comments)\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(driver.window_handles[0])\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt == 2:\n",
    "                            print(f\"Failed to extract post page {full_link}: {e}\")\n",
    "                            try:\n",
    "                                driver.close()\n",
    "                                driver.switch_to.window(driver.window_handles[0])\n",
    "                            except: pass\n",
    "                        else:\n",
    "                            time.sleep(2)\n",
    "                post_data = {\n",
    "                    \"Post ID\": post_id,\n",
    "                    \"Category\": post_cat,\n",
    "                    \"Post Title\": title,\n",
    "                    \"Post Author\": author,\n",
    "                    \"Post Date\": date,\n",
    "                    \"Post Content\": post_content,\n",
    "                    \"Support Count\": post_support,\n",
    "                    \"Total Number of Comments\": total_comment_count,\n",
    "                    \"Post URL\": full_link\n",
    "                }\n",
    "                all_posts.append(post_data)\n",
    "                existing_post_ids.add(post_id)\n",
    "                time.sleep(polite_delay)\n",
    "            nxt_li = soup.find(\"li\", class_=\"lia-paging-page-next\")\n",
    "            if nxt_li and nxt_li.find(\"a\"):\n",
    "                next_href = nxt_li.find(\"a\")[\"href\"]\n",
    "                url = \"https://forums.beyondblue.org.au\" + next_href if next_href.startswith(\"/\") else next_href\n",
    "            else:\n",
    "                break\n",
    "            if p % 5 == 0:\n",
    "                dfp = pd.DataFrame(all_posts)\n",
    "                dfp.sort_values(by=\"Post Date\", inplace=True)\n",
    "                dfp.to_csv(posts_csv, index=False)\n",
    "                dfc = pd.DataFrame(all_comments)\n",
    "                dfc.sort_values(by=\"Comment Date\", inplace=True)\n",
    "                dfc.to_csv(comments_csv, index=False)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    if all_posts:\n",
    "        dfp = pd.DataFrame(all_posts)\n",
    "        if os.path.exists(posts_csv):\n",
    "            dfp_existing = pd.read_csv(posts_csv)\n",
    "            dfp = pd.concat([dfp_existing, dfp], ignore_index=True)\n",
    "            dfp.drop_duplicates(subset=[\"Post ID\"], inplace=True)\n",
    "        dfp.sort_values(by=\"Post Date\", inplace=True)\n",
    "        dfp.to_csv(posts_csv, index=False)\n",
    "        print(f\"Saved {len(dfp)} posts to {posts_csv}\")\n",
    "    if all_comments:\n",
    "        dfc = pd.DataFrame(all_comments)\n",
    "        if os.path.exists(comments_csv):\n",
    "            dfc_existing = pd.read_csv(comments_csv)\n",
    "            dfc = pd.concat([dfc_existing, dfc], ignore_index=True)\n",
    "            dfc.drop_duplicates(subset=[\"Comment ID\"], inplace=True)\n",
    "        dfc.sort_values(by=\"Comment Date\", inplace=True)\n",
    "        dfc.to_csv(comments_csv, index=False)\n",
    "        print(f\"Saved {len(dfc)} comments to {comments_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mental_health_urls = {\n",
    "        \"anxiety\":      \"https://forums.beyondblue.org.au/t5/anxiety/bd-p/c1-sc2-b1?&sort=recent\",\n",
    "        \"depression\":   \"https://forums.beyondblue.org.au/t5/depression/bd-p/c1-sc2-b2?&sort=recent\",\n",
    "        \"ptsd_trauma\":  \"https://forums.beyondblue.org.au/t5/ptsd-and-trauma/bd-p/c1-sc2-b3?&sort=recent\",\n",
    "        \"suicidal_selfharm\": \"https://forums.beyondblue.org.au/t5/suicidal-thoughts-and-self-harm/bd-p/c1-sc2-b4?&sort=recent\",\n",
    "        \"staying_well\": \"https://forums.beyondblue.org.au/t5/staying-well/bd-p/c1-sc3-b1?&sort=recent\",\n",
    "        \"treatments\":   \"https://forums.beyondblue.org.au/t5/treatments-health-professionals/bd-p/c1-sc3-b2?&sort=recent\",\n",
    "        \"relationships\":\"https://forums.beyondblue.org.au/t5/relationship-and-family-issues/bd-p/c1-sc3-b3?&sort=recent\",\n",
    "        \"supporting_friends\": \"https://forums.beyondblue.org.au/t5/supporting-family-and-friends/bd-p/c1-sc3-b4?&sort=recent\",\n",
    "        \"long_term_support\":  \"https://forums.beyondblue.org.au/t5/long-term-support-over-the/bd-p/c1-sc3-b5?&sort=recent\",\n",
    "        \"young_people\": \"https://forums.beyondblue.org.au/t5/young-people/bd-p/c1-sc4-b1?&sort=recent\",\n",
    "        \"Sex_identity\": \"https://forums.beyondblue.org.au/t5/sexuality-and-gender-identity/bd-p/c1-sc4-b2?&sort=recent\",\n",
    "        \"Multiculture\":  \"https://forums.beyondblue.org.au/t5/multicultural-experiences/bd-p/c1-sc4-b3?&sort=recent\",\n",
    "        \"Grief_loss\":    \"https://forums.beyondblue.org.au/t5/grief-and-loss/bd-p/c1-sc4-b4?&sort=recent\"\n",
    "    }\n",
    "    for tag, addr in mental_health_urls.items():\n",
    "        try:\n",
    "            beyondblue_scraping(tag, addr, pages=100)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {tag}: {e}\")\n",
    "            continue"
   ],
   "id": "f036e56e181c9994",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping anxiety: 100%|██████████| 100/100 [1:37:48<00:00, 58.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1996 posts to Data\\posts_anxiety.csv\n",
      "Saved 998 comments to Data\\comments_anxiety.csv\n",
      "Error scraping depression: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x1271a33+62339]\n",
      "\tGetHandleVerifier [0x0x1271a74+62404]\n",
      "\t(No symbol) [0x0x10b2123]\n",
      "\t(No symbol) [0x0x10e58b8]\n",
      "\t(No symbol) [0x0x10e11a9]\n",
      "\t(No symbol) [0x0x112ae77]\n",
      "\t(No symbol) [0x0x112a76a]\n",
      "\t(No symbol) [0x0x111f1b6]\n",
      "\t(No symbol) [0x0x10ee7a2]\n",
      "\t(No symbol) [0x0x10ef644]\n",
      "\tGetHandleVerifier [0x0x14e65c3+2637587]\n",
      "\tGetHandleVerifier [0x0x14e19ca+2618138]\n",
      "\tGetHandleVerifier [0x0x12984aa+220666]\n",
      "\tGetHandleVerifier [0x0x12888d8+156200]\n",
      "\tGetHandleVerifier [0x0x128f06d+182717]\n",
      "\tGetHandleVerifier [0x0x1279978+94920]\n",
      "\tGetHandleVerifier [0x0x1279b02+95314]\n",
      "\tGetHandleVerifier [0x0x1264c4a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76015d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7770d1ab+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x7770d131+561]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping ptsd_trauma: 100%|██████████| 100/100 [1:54:51<00:00, 68.92s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 posts to Data\\posts_ptsd_trauma.csv\n",
      "Saved 1000 comments to Data\\comments_ptsd_trauma.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping suicidal_selfharm: 100%|██████████| 100/100 [1:59:52<00:00, 71.93s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 posts to Data\\posts_suicidal_selfharm.csv\n",
      "Saved 1000 comments to Data\\comments_suicidal_selfharm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping staying_well: 100%|██████████| 100/100 [3:08:35<00:00, 113.16s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 posts to Data\\posts_staying_well.csv\n",
      "Saved 1000 comments to Data\\comments_staying_well.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping treatments: 100%|██████████| 100/100 [1:35:54<00:00, 57.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 posts to Data\\posts_treatments.csv\n",
      "Saved 1000 comments to Data\\comments_treatments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping relationships: 100%|██████████| 100/100 [1:34:05<00:00, 56.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 posts to Data\\posts_relationships.csv\n",
      "Saved 1000 comments to Data\\comments_relationships.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping supporting_friends: 100%|██████████| 100/100 [1:37:48<00:00, 58.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 posts to Data\\posts_supporting_friends.csv\n",
      "Saved 1000 comments to Data\\comments_supporting_friends.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping long_term_support:   9%|▉         | 9/100 [1:58:51<20:01:52, 792.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 396\u001B[39m\n\u001B[32m    394\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m tag, addr \u001B[38;5;129;01min\u001B[39;00m mental_health_urls.items():\n\u001B[32m    395\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m396\u001B[39m         \u001B[43mbeyondblue_scraping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtag\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maddr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpages\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    397\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    398\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError scraping \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtag\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 311\u001B[39m, in \u001B[36mbeyondblue_scraping\u001B[39m\u001B[34m(tag, start_url, pages, polite_delay)\u001B[39m\n\u001B[32m    309\u001B[39m         date = standardize_date(time_elem.text.strip())\n\u001B[32m    310\u001B[39m \u001B[38;5;66;03m# Scrape all comments (not for count, but real data)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m311\u001B[39m comments = \u001B[43mcomment_scraping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdriver\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_link\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpost_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtag\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_comments\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    312\u001B[39m new_comments = []\n\u001B[32m    313\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m comments:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 183\u001B[39m, in \u001B[36mcomment_scraping\u001B[39m\u001B[34m(driver, post_url, post_id, category, max_comments, retry, polite_delay)\u001B[39m\n\u001B[32m    181\u001B[39m content_elem = msg.find(\u001B[33m\"\u001B[39m\u001B[33mdiv\u001B[39m\u001B[33m\"\u001B[39m, class_=\u001B[33m\"\u001B[39m\u001B[33mlia-message-body-content\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    182\u001B[39m comment_content = convert_emojis_emoticons(content_elem.get_text(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, strip=\u001B[38;5;28;01mTrue\u001B[39;00m)) \u001B[38;5;28;01mif\u001B[39;00m content_elem \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m183\u001B[39m comment_id = \u001B[43mmake_comment_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpost_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomment_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m comment_id \u001B[38;5;129;01min\u001B[39;00m existing_comment_ids:\n\u001B[32m    185\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 120\u001B[39m, in \u001B[36mmake_comment_id\u001B[39m\u001B[34m(msg, post_id, comment_content)\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m comment_id \u001B[38;5;129;01mand\u001B[39;00m comment_id.lower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mlineardisplaymessageviewwrapper\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m]:\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m comment_id\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m hash_part = \u001B[43mhashlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43msha256\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcomment_content\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m.hexdigest()[:\u001B[32m10\u001B[39m]\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpost_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_c\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhash_part\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d75d57dc44956f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T23:16:38.348821Z",
     "start_time": "2025-07-19T18:33:39.937608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import calendar\n",
    "import json\n",
    "import emoji\n",
    "import emot\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import unicodedata\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "KAOMOJI_FILE = \"kaomoji_to_text.json\"\n",
    "if os.path.exists(KAOMOJI_FILE):\n",
    "    with open(KAOMOJI_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        kaomoji_to_text = json.load(f)\n",
    "else:\n",
    "    kaomoji_to_text = {}\n",
    "\n",
    "OUTPUT_DIR = \"Data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = ''.join(c for c in s if unicodedata.category(c)[0] != 'C' and c != '\\uFFFD')\n",
    "    return s.replace('\\xa0', ' ').replace('\\u200e', '').strip()\n",
    "\n",
    "\n",
    "def standardize_date(raw_date: str) -> str:\n",
    "    s = clean_text(raw_date)\n",
    "    today = datetime.now()\n",
    "    weekdays = [d.lower() for d in calendar.day_name]\n",
    "    s_lower = s.lower()\n",
    "\n",
    "    # Handle relative dates\n",
    "    if s_lower in weekdays:\n",
    "        delta = (today.weekday() - weekdays.index(s_lower)) % 7\n",
    "        return (today - timedelta(days=delta)).strftime(\"%d-%m-%Y\")\n",
    "    if \"yesterday\" in s_lower:\n",
    "        return (today - timedelta(days=1)).strftime(\"%d-%m-%Y\")\n",
    "    if \"today\" in s_lower:\n",
    "        return today.strftime(\"%d-%m-%Y\")\n",
    "    if \"week\" in s_lower:\n",
    "        n = int(re.search(r\"(\\d+)\", s_lower).group(1)) if re.search(r\"\\d+\", s_lower) else 1\n",
    "        return (today - timedelta(weeks=n)).strftime(\"%d-%m-%Y\")\n",
    "    if \"month\" in s_lower:\n",
    "        n = int(re.search(r\"(\\d+)\", s_lower).group(1)) if re.search(r\"\\d+\", s_lower) else 1\n",
    "        return (today - timedelta(days=30 * n)).strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    # Try most relevant date formats\n",
    "    fmts = [\n",
    "        \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "        \"%Y-%m-%d %H:%M\",\n",
    "        \"%d-%m-%Y %H:%M\",\n",
    "        \"%d-%m-%Y %I:%M %p\",\n",
    "        \"%d/%m/%Y %H:%M\",\n",
    "        \"%d/%m/%Y %I:%M %p\",\n",
    "        \"%d-%m-%Y%H:%M\",\n",
    "        \"%Y-%m-%d%H:%M\",\n",
    "        \"%d/%m/%Y%H:%M\",\n",
    "    ]\n",
    "    for fmt in fmts:\n",
    "        try:\n",
    "            dt = datetime.strptime(s, fmt)\n",
    "            return dt.strftime(\"%d-%m-%Y\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    # Date only formats (no time info)\n",
    "    date_fmts = [\n",
    "        \"%d-%m-%Y\", \"%Y-%m-%d\", \"%d/%m/%Y\"\n",
    "    ]\n",
    "    for fmt in date_fmts:\n",
    "        try:\n",
    "            dt = datetime.strptime(s, fmt)\n",
    "            return dt.strftime(\"%d-%m-%Y\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    # Fallback: regex for non-standard\n",
    "    m = re.match(r\"(\\d{2})-(\\d{2})-(\\d{4})\", s)\n",
    "    if m:\n",
    "        return f\"{m.group(1)}-{m.group(2)}-{m.group(3)}\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def convert_emojis_emoticons(text: str) -> str:\n",
    "    text = clean_text(text)\n",
    "    for k, v in kaomoji_to_text.items():\n",
    "        text = text.replace(k, f\" {v} \")\n",
    "    e = emot.core.emot()\n",
    "    emo = e.emoticons(text)\n",
    "    for orig, mean in zip(emo[\"value\"], emo[\"mean\"]):\n",
    "        text = text.replace(orig, f\" {mean} \")\n",
    "    text = emoji.demojize(text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "def get_existing_ids(filepath, id_column):\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            return set(df[id_column].astype(str))\n",
    "        except Exception:\n",
    "            return set()\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "\n",
    "def make_comment_id(msg, post_id, comment_content):\n",
    "    comment_id = (msg.get('data-message-id') or msg.get('id') or '').strip()\n",
    "    if comment_id and comment_id.lower() not in ['lineardisplaymessageviewwrapper', '']:\n",
    "        return comment_id\n",
    "    hash_part = hashlib.sha256(comment_content.encode('utf-8')).hexdigest()[:10]\n",
    "    return f\"{post_id}_c{hash_part}\"\n",
    "\n",
    "\n",
    "def extract_comment_date(msg):\n",
    "    time_elem = msg.find(\"time\")\n",
    "    if time_elem:\n",
    "        if time_elem.has_attr('datetime'):\n",
    "            return standardize_date(time_elem['datetime'])\n",
    "        elif time_elem.has_attr('title'):\n",
    "            return standardize_date(time_elem['title'])\n",
    "        elif time_elem.text.strip():\n",
    "            return standardize_date(time_elem.text)\n",
    "    date_elem = msg.find(\"span\", class_=\"local-friendly-date\")\n",
    "    if date_elem:\n",
    "        if date_elem.has_attr('title') and date_elem['title'].strip():\n",
    "            return standardize_date(date_elem['title'])\n",
    "        elif date_elem.text.strip():\n",
    "            return standardize_date(date_elem.text)\n",
    "    datetime_elem = msg.find(\"span\", class_=\"DateTime\")\n",
    "    if datetime_elem and datetime_elem.text.strip():\n",
    "        return standardize_date(datetime_elem.text)\n",
    "    if msg.has_attr('data-message-timestamp'):\n",
    "        try:\n",
    "            ts = int(msg['data-message-timestamp'])\n",
    "            return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M')\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ''\n",
    "\n",
    "\n",
    "def comment_scraping(driver, post_url, post_id, category, max_comments=200, retry=3, polite_delay=1):\n",
    "    comments = []\n",
    "    comments_csv = os.path.join(OUTPUT_DIR, f\"comments_{category}.csv\")\n",
    "    existing_comment_ids = get_existing_ids(comments_csv, \"Comment ID\")\n",
    "    url = post_url\n",
    "    scraped = 0\n",
    "    for page in range(1, 100):  # will break on next not found\n",
    "        if scraped >= max_comments:\n",
    "            break\n",
    "        success = False\n",
    "        for attempt in range(retry):\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"linear-message-list\"))\n",
    "                )\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                success = True\n",
    "                break\n",
    "            except Exception:\n",
    "                time.sleep(2)\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        section = soup.find(\"div\", class_=\"linear-message-list\")\n",
    "        if not section:\n",
    "            break\n",
    "\n",
    "        msgs = section.find_all(\"div\", class_=\"lia-message-view-display\")\n",
    "        for msg in msgs:\n",
    "            if scraped >= max_comments:\n",
    "                break\n",
    "            content_elem = msg.find(\"div\", class_=\"lia-message-body-content\")\n",
    "            comment_content = convert_emojis_emoticons(content_elem.get_text(\"\\n\", strip=True)) if content_elem else \"\"\n",
    "            comment_id = make_comment_id(msg, post_id, comment_content)\n",
    "            if comment_id in existing_comment_ids:\n",
    "                continue\n",
    "            author_elem = msg.find(\"a\", class_=\"lia-user-name-link\")\n",
    "            comment_author = clean_text(author_elem.get_text(strip=True)) if author_elem else \"\"\n",
    "            comment_date = extract_comment_date(msg)\n",
    "            support_elem = msg.find(\"span\", {\"id\": re.compile(r\"^kudos-count-\")})\n",
    "            if not support_elem:\n",
    "                support_elem = msg.find(\"span\", class_=\"lia-component-kudos-widget-message-kudos-count\")\n",
    "            comment_support = support_elem.text.strip() if support_elem else \"0\"\n",
    "            comment_row = {\n",
    "                \"Comment ID\": comment_id,\n",
    "                \"Post ID\": post_id,\n",
    "                \"Category\": category,\n",
    "                \"Comment Author\": comment_author,\n",
    "                \"Comment Date\": comment_date,\n",
    "                \"Comment Content\": comment_content,\n",
    "                \"Comment Support\": comment_support,\n",
    "                \"Post URL\": post_url\n",
    "            }\n",
    "            comments.append(comment_row)\n",
    "            existing_comment_ids.add(comment_id)\n",
    "            scraped += 1\n",
    "        nxt = soup.find(\"a\", rel=\"next\")\n",
    "        if not nxt or not nxt.get(\"href\"):\n",
    "            break\n",
    "        url = \"https://forums.beyondblue.org.au\" + nxt[\"href\"] if nxt[\"href\"].startswith(\"/\") else nxt[\"href\"]\n",
    "        time.sleep(polite_delay)\n",
    "    comments = sorted(comments, key=lambda c: c[\"Comment Date\"])\n",
    "    return comments\n",
    "\n",
    "\n",
    "def beyondblue_scraping(tag: str, start_url: str, pages: int = 20, polite_delay=2):\n",
    "    posts_csv = os.path.join(OUTPUT_DIR, f\"posts_{tag}.csv\")\n",
    "    comments_csv = os.path.join(OUTPUT_DIR, f\"comments_{tag}.csv\")\n",
    "    existing_post_ids = get_existing_ids(posts_csv, \"Post ID\")\n",
    "    existing_comment_ids = get_existing_ids(comments_csv, \"Comment ID\")\n",
    "    all_posts = []\n",
    "    all_comments = []\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.set_page_load_timeout(30)\n",
    "    url = start_url\n",
    "    try:\n",
    "        for p in tqdm(range(1, pages + 1), desc=f\"Scraping {tag}\"):\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"custom-message-list\"))\n",
    "                    )\n",
    "                    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                    break\n",
    "                except Exception:\n",
    "                    if attempt == 2:\n",
    "                        print(f\"Failed to load page {url}\")\n",
    "                        return\n",
    "                    time.sleep(2)\n",
    "            container = soup.find(\"div\", class_=\"custom-message-list all-discussions\")\n",
    "            if not container:\n",
    "                break\n",
    "            for art in container.find_all(\"article\"):\n",
    "                aside = art.find(\"aside\")\n",
    "                cat_div = aside.find(\"div\", class_=\"custom-tile-category-content\") if aside else None\n",
    "                post_cat = clean_text(cat_div.find(\"a\").text.strip()) if cat_div and cat_div.find(\"a\") else tag\n",
    "                # --- DATE ---\n",
    "                time_el = cat_div.find(\"time\") if cat_div else None\n",
    "                if time_el and time_el.has_attr('datetime'):\n",
    "                    raw_date = time_el['datetime']\n",
    "                elif time_el and time_el.has_attr('title'):\n",
    "                    raw_date = time_el['title']\n",
    "                elif time_el:\n",
    "                    raw_date = time_el.text.strip()\n",
    "                else:\n",
    "                    raw_date = \"\"\n",
    "                date = standardize_date(raw_date)\n",
    "                # --- SUPPORT AND REPLIES ---\n",
    "                support_li = art.find(\"li\", class_=\"custom-tile-kudos\")\n",
    "                support_span = support_li.find(\"span\") if support_li else None\n",
    "                post_support = support_span.text.strip() if support_span else \"0\"\n",
    "\n",
    "                replies_li = art.find(\"li\", class_=\"custom-tile-replies\")\n",
    "                replies_b = replies_li.find(\"b\") if replies_li else None\n",
    "                total_comment_count = replies_b.text.strip() if replies_b else \"0\"\n",
    "                # --- POST TITLE/URL/CONTENT ---\n",
    "                h3 = art.find(\"h3\")\n",
    "                link_els = h3.find_all(\"a\") if h3 else []\n",
    "                link_el = link_els[1] if len(link_els) > 1 else (link_els[0] if link_els else None)\n",
    "                post_link = link_el[\"href\"] if link_el and link_el.has_attr(\"href\") else \"\"\n",
    "                post_id = post_link.rstrip(\"/\").split(\"/\")[-1] if post_link else \"\"\n",
    "                if not post_id or post_id in existing_post_ids:\n",
    "                    continue\n",
    "                full_link = \"https://forums.beyondblue.org.au\" + post_link if post_link.startswith(\"/\") else post_link\n",
    "                title = convert_emojis_emoticons(link_el.text.strip()) if link_el else \"\"\n",
    "                body = art.find(\"p\", class_=\"body-text\")\n",
    "                content = convert_emojis_emoticons(body.text.strip()) if body else \"\"\n",
    "                auth_div = aside.find(\"div\", class_=\"custom-tile-author-info\") if aside else None\n",
    "                auth_a = auth_div.find(\"a\") if auth_div else None\n",
    "                author = clean_text(auth_a.get_text(strip=True)) if auth_a else \"\"\n",
    "                post_content = content\n",
    "                # --- Get real post support from detail page ---\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        driver.execute_script(\"window.open('');\")\n",
    "                        driver.switch_to.window(driver.window_handles[1])\n",
    "                        driver.get(full_link)\n",
    "                        WebDriverWait(driver, 10).until(\n",
    "                            EC.presence_of_element_located((By.CLASS_NAME, \"lia-message-body-content\"))\n",
    "                        )\n",
    "                        post_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                        content_elems = post_soup.find_all(\"div\", class_=\"lia-message-body-content\")\n",
    "                        if content_elems:\n",
    "                            post_content = \"\\n\".join(\n",
    "                                [convert_emojis_emoticons(elem.get_text(\"\\n\", strip=True)) for elem in content_elems if\n",
    "                                 elem.get_text(strip=True)])\n",
    "                        # Get support count from first post in thread\n",
    "                        support_span_detail = post_soup.find(\"span\", {\"id\": re.compile(r\"^kudos-count-\")})\n",
    "                        if not support_span_detail:\n",
    "                            support_span_detail = post_soup.find(\"span\",\n",
    "                                                                 class_=\"lia-component-kudos-widget-message-kudos-count\")\n",
    "                        if support_span_detail:\n",
    "                            post_support = support_span_detail.text.strip()\n",
    "                        # Get post date from detail if missing\n",
    "                        time_elem = post_soup.find(\"time\")\n",
    "                        if not date and time_elem:\n",
    "                            if time_elem.has_attr(\"datetime\"):\n",
    "                                date = standardize_date(time_elem[\"datetime\"])\n",
    "                            elif time_elem.has_attr(\"title\"):\n",
    "                                date = standardize_date(time_elem[\"title\"])\n",
    "                            else:\n",
    "                                date = standardize_date(time_elem.text.strip())\n",
    "                        # Scrape all comments (not for count, but real data)\n",
    "                        comments = comment_scraping(driver, full_link, post_id, tag, max_comments=200)\n",
    "                        new_comments = []\n",
    "                        for c in comments:\n",
    "                            if c[\"Comment ID\"] not in existing_comment_ids:\n",
    "                                new_comments.append(c)\n",
    "                                existing_comment_ids.add(c[\"Comment ID\"])\n",
    "                        all_comments.extend(new_comments)\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(driver.window_handles[0])\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt == 2:\n",
    "                            print(f\"Failed to extract post page {full_link}: {e}\")\n",
    "                            try:\n",
    "                                driver.close()\n",
    "                                driver.switch_to.window(driver.window_handles[0])\n",
    "                            except:\n",
    "                                pass\n",
    "                        else:\n",
    "                            time.sleep(2)\n",
    "                post_data = {\n",
    "                    \"Post ID\": post_id,\n",
    "                    \"Category\": post_cat,\n",
    "                    \"Post Title\": title,\n",
    "                    \"Post Author\": author,\n",
    "                    \"Post Date\": date,\n",
    "                    \"Post Content\": post_content,\n",
    "                    \"Support Count\": post_support,\n",
    "                    \"Total Number of Comments\": total_comment_count,\n",
    "                    \"Post URL\": full_link\n",
    "                }\n",
    "                all_posts.append(post_data)\n",
    "                existing_post_ids.add(post_id)\n",
    "                time.sleep(polite_delay)\n",
    "            nxt_li = soup.find(\"li\", class_=\"lia-paging-page-next\")\n",
    "            if nxt_li and nxt_li.find(\"a\"):\n",
    "                next_href = nxt_li.find(\"a\")[\"href\"]\n",
    "                url = \"https://forums.beyondblue.org.au\" + next_href if next_href.startswith(\"/\") else next_href\n",
    "            else:\n",
    "                break\n",
    "            if p % 5 == 0:\n",
    "                dfp = pd.DataFrame(all_posts)\n",
    "                dfp.sort_values(by=\"Post Date\", inplace=True)\n",
    "                dfp.to_csv(posts_csv, index=False)\n",
    "                dfc = pd.DataFrame(all_comments)\n",
    "                dfc.sort_values(by=\"Comment Date\", inplace=True)\n",
    "                dfc.to_csv(comments_csv, index=False)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    if all_posts:\n",
    "        dfp = pd.DataFrame(all_posts)\n",
    "        if os.path.exists(posts_csv):\n",
    "            dfp_existing = pd.read_csv(posts_csv)\n",
    "            dfp = pd.concat([dfp_existing, dfp], ignore_index=True)\n",
    "            dfp.drop_duplicates(subset=[\"Post ID\"], inplace=True)\n",
    "        dfp.sort_values(by=\"Post Date\", inplace=True)\n",
    "        dfp.to_csv(posts_csv, index=False)\n",
    "        print(f\"Saved {len(dfp)} posts to {posts_csv}\")\n",
    "    if all_comments:\n",
    "        dfc = pd.DataFrame(all_comments)\n",
    "        if os.path.exists(comments_csv):\n",
    "            dfc_existing = pd.read_csv(comments_csv)\n",
    "            dfc = pd.concat([dfc_existing, dfc], ignore_index=True)\n",
    "            dfc.drop_duplicates(subset=[\"Comment ID\"], inplace=True)\n",
    "        dfc.sort_values(by=\"Comment Date\", inplace=True)\n",
    "        dfc.to_csv(comments_csv, index=False)\n",
    "        print(f\"Saved {len(dfc)} comments to {comments_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mental_health_urls = {\n",
    "        \"depression\": \"https://forums.beyondblue.org.au/t5/depression/bd-p/c1-sc2-b2?&sort=recent\",\n",
    "        \"long_term_support\": \"https://forums.beyondblue.org.au/t5/long-term-support-over-the/bd-p/c1-sc3-b5?&sort=recent\",\n",
    "        \"young_people\": \"https://forums.beyondblue.org.au/t5/young-people/bd-p/c1-sc4-b1?&sort=recent\",\n",
    "        \"Sex_identity\": \"https://forums.beyondblue.org.au/t5/sexuality-and-gender-identity/bd-p/c1-sc4-b2?&sort=recent\",\n",
    "        \"Multiculture\": \"https://forums.beyondblue.org.au/t5/multicultural-experiences/bd-p/c1-sc4-b3?&sort=recent\",\n",
    "        \"Grief_loss\": \"https://forums.beyondblue.org.au/t5/grief-and-loss/bd-p/c1-sc4-b4?&sort=recent\"\n",
    "    }\n",
    "    for tag, addr in mental_health_urls.items():\n",
    "        try:\n",
    "            beyondblue_scraping(tag, addr, pages=100)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {tag}: {e}\")\n",
    "            continue"
   ],
   "id": "3bef05acffade133",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping depression:   4%|▍         | 4/100 [00:11<04:35,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping depression: 'Post Date'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping long_term_support:   4%|▍         | 4/100 [00:08<03:18,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping long_term_support: 'Post Date'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping young_people: 100%|██████████| 100/100 [1:29:55<00:00, 53.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 posts to Data\\posts_young_people.csv\n",
      "Saved 1000 comments to Data\\comments_young_people.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Sex_identity:  77%|███████▋  | 77/100 [1:17:49<23:14, 60.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1526 posts to Data\\posts_Sex_identity.csv\n",
      "Saved 776 comments to Data\\comments_Sex_identity.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Multiculture:  25%|██▌       | 25/100 [29:04<1:27:14, 69.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 506 posts to Data\\posts_Multiculture.csv\n",
      "Saved 256 comments to Data\\comments_Multiculture.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Grief_loss:  87%|████████▋ | 87/100 [1:25:10<12:43, 58.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1725 posts to Data\\posts_Grief_loss.csv\n",
      "Saved 875 comments to Data\\comments_Grief_loss.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bbdb9084e9352a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
